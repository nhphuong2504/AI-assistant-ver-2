# AI analyst for customer growth & retention â€“ Retail Data Chat Application

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![Node.js](https://img.shields.io/badge/node.js-18+-green.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-009688.svg)
![React](https://img.shields.io/badge/React-18-61DAFB.svg)
![TypeScript](https://img.shields.io/badge/TypeScript-5.0+-3178C6.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)

A full-stack monorepo with a FastAPI backend and React frontend that delivers AI-powered retail analytics, conversational insights, and customer growth & retention intelligence.

## ğŸ”„ Previous Version
> 
> **[ğŸ”— Legacy Repository](https://github.com/nhphuong2504/AI-assistant)**

## ğŸ¥ Demo

<a href="https://youtu.be/0CBPu7cR1ss">
  <img src="video/thumbnail.png" width="800" alt="Demo Video" />
</a>


## âœ¨ Key Features

### Basic Features

- **ğŸ¤– AI-Powered Chat Interface** - Ask questions about your retail data in natural language
- **ğŸ’¬ Conversation Memory** - Context-aware responses with thread-based conversation history
- **ğŸ” SQL Query Interface** - Direct database querying with read-only access for data exploration
- **ğŸ“Š Database Schema Exploration** - View and understand your database structure through the API
- **ğŸŒ Modern Web Interface** - Beautiful React-based UI with real-time chat interactions
- **âš¡ Fast API Responses** - FastAPI backend with async support for quick query processing
- **ğŸ”’ CORS Enabled** - Seamless frontend-backend communication configured out of the box

## ğŸ“ˆ Advanced Insight Analytics

The application includes sophisticated analytics modules for deep customer insights:

### Customer Lifetime Value (CLV) Analysis

- **BG/NBD Model** - Predicts customer purchase frequency and probability of being alive
- **Gamma-Gamma Model** - Estimates average order value per customer
- **RFM Analysis** - Recency, Frequency, Monetary value segmentation
- **CLV Prediction** - Forecast customer lifetime value over specified time horizons

### Survival Analysis & Churn Prediction

- **Cox Proportional Hazards Model** - Identifies factors affecting customer churn risk
- **Kaplan-Meier Estimation** - Non-parametric survival curve analysis
- **Customer Risk Scoring** - Leakage-free risk scoring for active customers
- **Expected Remaining Lifetime (ERL)** - Monte Carlo simulation (BG/NBD) predicts expected days until churn; 
- **Churn Probability** - Estimates probability of churn within specified time horizons 

### Customer Segmentation

- **Risk-Based Segmentation** - High/Medium/Low risk from Cox model
- **ERL-Based Buckets** - Fixed thresholds: **At-Risk** (0â€“90 days), **Stable** (91â€“270), **Valued** (271â€“720), **VIP** (>720 days)
- **12 Segments** - Combines risk (High/Medium/Low) and ERL bucket (At-Risk, Stable, Valued, VIP), e.g. `High/At-Risk`, `Medium/Valued`, `Low/VIP`
- **Actionable Recommendations** - Segment-specific action tags and recommended strategies 


## ğŸ“ Project Structure

```
AI-assistant-ver-2/
â”œâ”€â”€ backend/              # FastAPI backend application
â”‚   â”œâ”€â”€ app/             # Main application code
â”‚   â”‚   â”œâ”€â”€ main.py      # FastAPI routes and endpoints
â”‚   â”‚   â”œâ”€â”€ db.py        # Database utilities
â”‚   â”‚   â”œâ”€â”€ data.py      # Shared data cache (transactions, CLV models)
â”‚   â”‚   â””â”€â”€ llm_langchain.py  # LangChain AI agent & Cox model cache
â”‚   â”œâ”€â”€ analytics/       # Analytics modules (CLV, survival, Monte Carlo ERL)
â”‚   â”œâ”€â”€ etl/             # Data loading scripts
â”‚   â”œâ”€â”€ data/            # Data files (CSV, SQLite)
â”‚   â”œâ”€â”€ requirements.txt # Python dependencies
â”‚   â””â”€â”€ test/            # Backend tests
â”œâ”€â”€ frontend/            # React + TypeScript frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/  # React components
â”‚   â”‚   â”œâ”€â”€ pages/       # Page components
â”‚   â”‚   â””â”€â”€ lib/         # API utilities
â”‚   â””â”€â”€ public/          # Static assets
â”œâ”€â”€ venv/                # Python virtual environment (root level)
â”œâ”€â”€ setup.bat            # Backend setup script (Windows)
â”œâ”€â”€ setup.sh             # Backend setup script (Linux/Mac)
â”œâ”€â”€ start-backend.bat    # Backend startup script (Windows)
â”œâ”€â”€ start-backend.sh     # Backend startup script (Linux/Mac)
â”œâ”€â”€ start-frontend.bat   # Frontend startup script (Windows)
â””â”€â”€ start-frontend.sh    # Frontend startup script (Linux/Mac)
```

## ğŸ—ï¸ Architecture

### System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    HTTP/REST API    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   React      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  FastAPI     â”‚
â”‚  Frontend    â”‚                      â”‚   Backend    â”‚
â”‚  (Port 8080) â”‚                      â”‚  (Port 8000) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                        â”‚            â”‚
                    â–¼                        â–¼            â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   SQLite     â”‚      â”‚  Analytics   â”‚  â”‚  OpenAI  â”‚
            â”‚  Database    â”‚      â”‚   Modules    â”‚  â”‚    API   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Backend Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FastAPI Backend                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         API Layer (main.py)                 â”‚  â”‚
â”‚  â”‚  /ask-langchain  /query  /schema  /clv       â”‚  â”‚
â”‚  â”‚  /survival/score  /churn-probability  etc.  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                     â”‚                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚      Shared Caches (app/data.py)             â”‚  â”‚
â”‚  â”‚  â€¢ get_transactions_df()  â€¢ get_clv_models() â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                     â”‚                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚      LangChain Agent (llm_langchain.py)      â”‚  â”‚
â”‚  â”‚  â€¢ Tool selection  â€¢ Cox model cache        â”‚  â”‚
â”‚  â”‚  â€¢ Memory  â€¢ Token usage logging            â”‚  â”‚
â”‚  â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚     â–¼          â–¼          â–¼          â–¼             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ SQL  â”‚  â”‚  CLV   â”‚  â”‚ Churn  â”‚  â”‚Segmentationâ”‚ â”‚
â”‚  â”‚Query â”‚  â”‚Module  â”‚  â”‚Module  â”‚  â”‚  Module   â”‚  â”‚
â”‚  â””â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â–¼           â–¼           â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SQLite   â”‚ â”‚ Pandas   â”‚ â”‚ Pandas   â”‚ â”‚ Pandas   â”‚
â”‚ Database â”‚ â”‚DataFramesâ”‚ â”‚DataFramesâ”‚ â”‚DataFramesâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
User Question
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ChatContainerâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Client  â”‚ POST /ask-langchain
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LangChain    â”‚ Analyze â†’ Select Tool
â”‚ Agent        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”
   â”‚       â”‚
   â–¼       â–¼
â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SQL â”‚ â”‚Analytics â”‚
â”‚Queryâ”‚ â”‚Functions â”‚
â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
   â”‚         â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OpenAI LLM  â”‚ Generate Response
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API Response â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ChatMessage  â”‚ Display Answer
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Dataset

This application uses the **Online Retail** dataset from the UCI Machine Learning Repository. This is a transactional data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.

**Dataset Source:** [UCI Machine Learning Repository - Online Retail](https://archive.ics.uci.edu/dataset/352/online+retail)

The raw CSV data is located at `backend/data/raw/online_retail.csv` and is processed into a SQLite database during setup.

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.8+** installed
- **Node.js 18+** and npm installed
- **OpenAI API Key** (for AI features)

### Backend Setup

1. **Run the setup script:**
   
   **Windows:**
   ```bash
   setup.bat
   ```
   
   **Linux/Mac:**
   ```bash
   chmod +x setup.sh
   ./setup.sh
   ```
   
   This will:
   - Create a Python virtual environment at the root (`venv/`)
   - Install all Python dependencies from `backend/requirements.txt`

2. **Configure environment variables:**
   - Copy `backend/.env.example` to `backend/.env`
   - Add your OpenAI API key:
     ```
     OPENAI_API_KEY=your_api_key_here
     OPENAI_MODEL=gpt-4o-mini
     API_URL=http://127.0.0.1:8000
     DATABASE_PATH=backend/data/retail.sqlite
     ```

3. **Generate the database:**
   The `retail.sqlite` database file is not included in the repository. You need to generate it from the CSV data:
   
   **Windows:**
   ```bash
   cd backend
   ..\venv\Scripts\python etl\load_online_retail.py
   ```
   
   **Linux/Mac:**
   ```bash
   cd backend
   ../venv/bin/python etl/load_online_retail.py
   ```
   
   This script will:
   - Read the CSV file from `backend/data/raw/online_retail.csv`
   - Clean and process the data
   - Create `backend/data/retail.sqlite` with two tables:
     - `transactions_all` - All raw transactions with data quality flags
     - `transactions` - Clean transactions for analytics (excludes cancellations, returns, invalid data)
   - Create indexes for optimal query performance

4. **Start the backend server:**
   
   **Windows:**
   ```bash
   start-backend.bat
   ```
   
   **Linux/Mac:**
   ```bash
   chmod +x start-backend.sh
   ./start-backend.sh
   ```
   
   The backend will run on `http://127.0.0.1:8000`

### Frontend Setup

1. **Start the frontend development server:**
   
   **Windows:**
   ```bash
   start-frontend.bat
   ```
   
   **Linux/Mac:**
   ```bash
   chmod +x start-frontend.sh
   ./start-frontend.sh
   ```
   
   This will automatically install Node.js dependencies if needed and start the dev server.


The frontend will run on `http://localhost:8080`

## ğŸ Virtual Environment

### Manual Activation (if needed)

**Windows:**
```bash
venv\Scripts\activate
```

**Linux/Mac:**
```bash
source venv/bin/activate
```

After activation, you can run Python commands directly. The setup and startup scripts handle activation automatically.

## ğŸ”§ Technology Stack

### Backend
- **FastAPI** - Modern Python web framework
- **LangChain** - AI agent framework
- **LangGraph** - Agent orchestration
- **LangChain Community** - Token usage callbacks (OpenAI)
- **OpenAI** - LLM integration
- **SQLite** - Database
- **Pandas** - Data manipulation
- **Uvicorn** - ASGI server
- **lifelines** - Survival analysis (Cox, Kaplan-Meier)
- **lifetimes** - CLV models (BG/NBD, Gamma-Gamma)

### Frontend
- **React 18** - UI framework
- **TypeScript** - Type safety
- **Vite** - Build tool
- **Tailwind CSS** - Styling
- **shadcn/ui** - UI components
- **React Router** - Routing
- **TanStack Query** - Data fetching

## ğŸ“¡ API Endpoints

### Main Endpoints

- `POST /ask-langchain` - Ask questions to the AI assistant (LangChain agent with tools; token usage logged per request)
  ```json
  {
    "question": "What are the top customers?",
    "use_memory": true,
    "thread_id": "default"
  }
  ```

- `POST /query` - Execute SQL queries (read-only)
- `GET /schema` - Get database schema
- `GET /health` - Health check
- `POST /ask-langchain/clear-memory` - Clear conversation memory

### Analytics Endpoints

- `POST /clv` - Customer Lifetime Value predictions (BG/NBD + Gamma-Gamma; cached by cutoff date)
- `POST /survival/km` - Kaplan-Meier survival curve
- `POST /survival/score` - Churn risk scoring
- `POST /survival/churn-probability` - Churn probability in next X days 
- `POST /survival/expected-lifetime` - Expected remaining lifetime in days (Monte Carlo ERL)
- `POST /survival/segmentation` - Risk + ERL segmentation 

See `backend/app/main.py` for request/response schemas and full API documentation.

## ğŸ› ï¸ Development

### Backend Development

The backend uses FastAPI with auto-reload enabled. Changes to Python files will automatically restart the server.

### Database Setup

If you need to regenerate the `retail.sqlite` database (e.g., after modifying the ETL script or CSV data):

**Windows:**
```bash
cd backend
..\venv\Scripts\python etl\load_online_retail.py
```

**Linux/Mac:**
```bash
cd backend
../venv/bin/python etl/load_online_retail.py
```

**Note:** The database file (`backend/data/retail.sqlite`) is excluded from version control via `.gitignore`. Each developer needs to generate it locally using the ETL script.

### Performance & Caching

The backend caches heavy computations to avoid redundant work:

- **Transactions** â€“ `app/data.py` caches the transactions DataFrame; all endpoints and the LangChain agent use `get_transactions_df()`.
- **CLV models** â€“ RFM + BG/NBD + Gamma-Gamma are cached by `cutoff_date` via `get_clv_models(cutoff_date)`; used by `/clv`, expected-lifetime, and LLM CLV/ERL/retention tools.
- **Cox model** â€“ Fitted Cox model is cached (1-hour TTL) in `llm_langchain.py` via `get_or_fit_cox_model()`; used by survival score, churn-probability, segmentation, and LLM risk/churn/segmentation tools.

### Frontend Development

The frontend uses Vite with hot module replacement. Changes to React components will update in the browser automatically.


## ğŸ“ Configuration

### Backend Environment Variables

Create `backend/.env` with:
- `OPENAI_API_KEY` - Your OpenAI API key (required)
- `OPENAI_MODEL` - Model to use (default: `gpt-4o-mini`)
- `API_URL` - Backend URL (default: `http://127.0.0.1:8000`)
- `DATABASE_PATH` - Path to SQLite database

### Frontend Environment Variables

Create `frontend/.env` with:
- `VITE_API_URL` - Backend API URL (default: `http://127.0.0.1:8000`)

âš ï¸ Never commit `.env` files. Add `backend/.env` and `frontend/.env` to `.gitignore`.


